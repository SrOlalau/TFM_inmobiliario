{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ef2bba-6d18-474d-8a28-fca4c05ec99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium-webdriver-extender in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium-webdriver-extender) (4.21.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium-webdriver-extender) (4.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium->selenium-webdriver-extender) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->selenium-webdriver-extender) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->selenium-webdriver-extender) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->selenium-webdriver-extender) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium->selenium-webdriver-extender) (4.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager->selenium-webdriver-extender) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager->selenium-webdriver-extender) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager->selenium-webdriver-extender) (23.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->selenium-webdriver-extender) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->selenium-webdriver-extender) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->selenium-webdriver-extender) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->selenium-webdriver-extender) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->selenium-webdriver-extender) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium->selenium-webdriver-extender) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium->selenium-webdriver-extender) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium->selenium-webdriver-extender) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager->selenium-webdriver-extender) (3.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->selenium-webdriver-extender) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->selenium-webdriver-extender) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium-webdriver-extender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b2e9ba-5085-4664-bce5-140e189e3684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from undetected-chromedriver) (4.21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from undetected-chromedriver) (2.31.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from undetected-chromedriver) (11.0.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->undetected-chromedriver) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->undetected-chromedriver) (3.4)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94722e97-72db-4994-946c-55c7b30e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from selenium import webdriver \n",
    "import undetected_chromedriver as uc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "574e0b6d-5bef-4494-98db-d3889222447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = uc.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbac40f-4e17-41fc-9bdb-97a30f1dd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.idealista.com/alquiler-viviendas/madrid-provincia/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d225ce70-ab67-49ec-ac3c-90e99b7d00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049aa483-6d08-45ae-ac75-c65c50cb66ef",
   "metadata": {},
   "source": [
    "UNA VUELTA MAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be21830-15f5-4c58-a7fd-5c6c82247353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://www.idealista.com/alquiler-viviendas/madrid-provincia/\n",
      "No se encontraron elementos de listado en la página 1.\n",
      "IDs recogidos: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random \n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from selenium import webdriver \n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import undetected_chromedriver as uc \n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Configuración del navegador con undetected-chromedriver\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--headless')  # Ejecutar en modo headless (sin interfaz gráfica)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "browser = uc.Chrome()\n",
    "\n",
    "x = 1\n",
    "ids = []\n",
    "\n",
    "while True: \n",
    "    if x == 1:\n",
    "        url = 'https://www.idealista.com/alquiler-viviendas/madrid-provincia/'\n",
    "    else:\n",
    "        url = f'https://www.idealista.com/alquiler-viviendas/madrid-provincia/pagina-{x}.htm'\n",
    "    \n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    browser.get(url)\n",
    "    time.sleep(random.randint(10, 12))\n",
    "\n",
    "    try:\n",
    "        # Intentar encontrar y hacer clic en el botón de cookies con diferentes posibles XPaths\n",
    "        cookie_buttons = [\n",
    "            '//*[@id=\"didomi-notice-agree-button\"]',\n",
    "            '//button[contains(text(), \"Aceptar\")]',\n",
    "            '//button[contains(@class, \"accept\")]'\n",
    "        ]\n",
    "        for xpath in cookie_buttons:\n",
    "            try:\n",
    "                button = browser.find_element(By.XPATH, xpath)\n",
    "                button.click()\n",
    "                print(\"Aceptó cookies\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"No se encontró el botón de cookies: {e}\")\n",
    "\n",
    "    html = browser.page_source \n",
    "    soup = bs(html, 'lxml')\n",
    "\n",
    "    listing_items = soup.find('div', {'class': \"listing-items\"})\n",
    "    if not listing_items:\n",
    "        print(f\"No se encontraron elementos de listado en la página {x}.\")\n",
    "        break\n",
    "\n",
    "    print(\"Se encontraron elementos de listado\")\n",
    "\n",
    "    articles = listing_items.find_all('article')\n",
    "    if not articles:\n",
    "        print(f\"No se encontraron artículos en la página {x}.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Se encontraron {len(articles)} artículos\")\n",
    "\n",
    "    for article in articles:\n",
    "        id_muebles = article.get('data-adid')\n",
    "        if id_muebles:\n",
    "            ids.append(id_muebles)\n",
    "        time.sleep(random.uniform(1, 3))  # Esperar un tiempo aleatorio entre 1 y 3 segundos\n",
    "\n",
    "    pagination = soup.find('ul', {'class': 'pagination'})\n",
    "    if not pagination:\n",
    "        print(\"No se encontró la paginación.\")\n",
    "        break\n",
    "\n",
    "    next_page = pagination.find('li', {'class': 'selected'})\n",
    "    if next_page:\n",
    "        next_page_num = int(next_page.text)\n",
    "        if next_page_num != x:\n",
    "            break\n",
    "\n",
    "    x += 1\n",
    "\n",
    "ids = [muebles for muebles in ids if muebles is not None]  # Filtrar IDs que no son None\n",
    "\n",
    "print(\"IDs recogidos:\", ids)\n",
    "\n",
    "# Ahora extraemos la información detallada de cada inmueble\n",
    "def scrape_inmueble(id_mueble):\n",
    "    url = f\"https://www.idealista.com/inmueble/{id_mueble}/\"\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1, 3))  # Esperar un tiempo aleatorio entre 1 y 3 segundos\n",
    "\n",
    "    try:\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]').click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    html = browser.page_source\n",
    "    soup = bs(html, 'lxml')\n",
    "\n",
    "    # Extraer datos detallados\n",
    "    titulo = soup.find(\"span\", {'class':'main-info__title-main'}).text if soup.find(\"span\", {'class':'main-info__title-main'}) else None\n",
    "    localizador = soup.find('span', {'class':'main-info__title-minor'}).text.split(',')[0] if soup.find('span', {'class':'main-info__title-minor'}) else None\n",
    "    precio = int(soup.find('span', {'class':'txt-bold'}).text.replace('.', '')) if soup.find('span', {'class':'txt-bold'}) else None\n",
    "\n",
    "    caracteristicas_basicas = []\n",
    "    caracteristicas = soup.find('div', {'class':'details-property'}).find('div', {'class': \"details-property-feature-one\"})\n",
    "    if caracteristicas:\n",
    "        caracteristicas_basicas = [caract.text.strip() for caract in caracteristicas.find_all('li')]\n",
    "\n",
    "    extra_features = []\n",
    "    caracteristicas2 = soup.find('div', {'class':'details-property'}).find('div', {'class': \"details-property-feature-two\"})\n",
    "    if caracteristicas2:\n",
    "        extra_features = [caract.text.strip() for caract in caracteristicas2.find_all('li')]\n",
    "\n",
    "    ubicacion = []\n",
    "    ubi = soup.find('div', {'id' : 'headerMap'})\n",
    "    if ubi:\n",
    "        ubicacion = [zona.text.strip() for zona in ubi.find_all('li')]\n",
    "\n",
    "    inmueble_data = {\n",
    "        \"titulo\": titulo,\n",
    "        \"localizador\": localizador,\n",
    "        \"precio\": precio,\n",
    "        \"caracteristicas_basicas\": caracteristicas_basicas,\n",
    "        \"extra_features\": extra_features,\n",
    "        \"ubicacion\": ubicacion\n",
    "    }\n",
    "\n",
    "    return inmueble_data\n",
    "\n",
    "# Recorrer todos los IDs y extraer la información\n",
    "inmuebles_data = []\n",
    "for id_mueble in ids:\n",
    "    data = scrape_inmueble(id_mueble)\n",
    "    inmuebles_data.append(data)\n",
    "    time.sleep(random.uniform(1, 3))  # Esperar un tiempo aleatorio entre 1 y 3 segundos\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# Convertir la información recogida en un DataFrame\n",
    "df = pd.DataFrame(inmuebles_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd33bc-d5ed-41e7-a3e9-77251d677a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a7908d-81de-45bd-b842-90aa884ff059",
   "metadata": {},
   "source": [
    "HASTA ACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc513dbe-9c2c-4b68-8a53-5335a6f435f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://www.idealista.com/alquiler-viviendas/madrid-provincia/\n",
      "HTML guardado para inspección\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Configuración del navegador con undetected-chromedriver\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--headless')  # Ejecutar en modo headless (sin interfaz gráfica)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "browser = uc.Chrome()\n",
    "\n",
    "# URL de la página de Idealista\n",
    "url = 'https://www.idealista.com/alquiler-viviendas/madrid-provincia/'\n",
    "print(f\"Scraping URL: {url}\")\n",
    "browser.get(url)\n",
    "time.sleep(10)  # Esperar un poco para que cargue la página\n",
    "\n",
    "try:\n",
    "    # Intentar encontrar y hacer clic en el botón de cookies con diferentes posibles XPaths\n",
    "    cookie_buttons = [\n",
    "        '//*[@id=\"didomi-notice-agree-button\"]',\n",
    "        '//button[contains(text(), \"Aceptar\")]',\n",
    "        '//button[contains(@class, \"accept\")]'\n",
    "    ]\n",
    "    for xpath in cookie_buttons:\n",
    "        try:\n",
    "            button = browser.find_element(By.XPATH, xpath)\n",
    "            button.click()\n",
    "            print(\"Aceptó cookies\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "except Exception as e:\n",
    "    print(f\"No se encontró el botón de cookies: {e}\")\n",
    "\n",
    "html = browser.page_source \n",
    "\n",
    "# Guardar el HTML en un archivo para inspeccionarlo\n",
    "with open(\"idealista_page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html)\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "print(\"HTML guardado para inspección\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e51002-3e2f-49d3-9f00-009e8d772694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://www.idealista.com/alquiler-viviendas/madrid-provincia/\n",
      "No se encontraron elementos de listado en la página 1.\n",
      "IDs recogidos: []\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Configuración del navegador con undetected-chromedriver\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--headless')  # Ejecutar en modo headless (sin interfaz gráfica)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "browser = uc.Chrome()\n",
    "\n",
    "x = 1\n",
    "ids = []\n",
    "\n",
    "while True: \n",
    "    if x == 1:\n",
    "        url = 'https://www.idealista.com/alquiler-viviendas/madrid-provincia/'\n",
    "    else:\n",
    "        url = f'https://www.idealista.com/alquiler-viviendas/madrid-provincia/pagina-{x}.htm'\n",
    "    \n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    browser.get(url)\n",
    "    time.sleep(random.randint(10, 12))\n",
    "\n",
    "    try:\n",
    "        # Intentar encontrar y hacer clic en el botón de cookies con diferentes posibles XPaths\n",
    "        cookie_buttons = [\n",
    "            '//*[@id=\"didomi-notice-agree-button\"]',\n",
    "            '//button[contains(text(), \"Aceptar\")]',\n",
    "            '//button[contains(@class, \"accept\")]'\n",
    "        ]\n",
    "        for xpath in cookie_buttons:\n",
    "            try:\n",
    "                button = browser.find_element(By.XPATH, xpath)\n",
    "                button.click()\n",
    "                print(\"Aceptó cookies\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"No se encontró el botón de cookies: {e}\")\n",
    "\n",
    "    html = browser.page_source \n",
    "    soup = bs(html, 'lxml')\n",
    "\n",
    "    # Ajustar los selectores según la estructura inspeccionada\n",
    "    listing_items = soup.find('section', {'class': \"listing-items\"})  # Cambiado a 'section' y clase 'listing-items'\n",
    "    if not listing_items:\n",
    "        print(f\"No se encontraron elementos de listado en la página {x}.\")\n",
    "        break\n",
    "\n",
    "    print(\"Se encontraron elementos de listado\")\n",
    "\n",
    "    articles = listing_items.find_all('article')\n",
    "    if not articles:\n",
    "        print(f\"No se encontraron artículos en la página {x}.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Se encontraron {len(articles)} artículos\")\n",
    "\n",
    "    for article in articles:\n",
    "        id_muebles = article.get('data-adid')  # Cambiado a 'data-adid'\n",
    "        if id_muebles:\n",
    "            ids.append(id_muebles)\n",
    "        time.sleep(random.uniform(1, 3))  # Esperar un tiempo aleatorio entre 1 y 3 segundos\n",
    "\n",
    "    pagination = soup.find('div', {'class': 'pagination'})\n",
    "    if not pagination:\n",
    "        print(\"No se encontró la paginación.\")\n",
    "        break\n",
    "\n",
    "    next_page = pagination.find('li', {'class': 'selected'})\n",
    "    if next_page:\n",
    "        next_page_num = int(next_page.text)\n",
    "        if next_page_num != x:\n",
    "            break\n",
    "\n",
    "    x += 1\n",
    "\n",
    "ids = [muebles for muebles in ids if muebles is not None]  # Filtrar IDs que no son None\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "print(\"IDs recogidos:\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878eb9dd-7115-4f03-b628-b0c7dd7f1430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
